{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vUhMqkcDnTK"
   },
   "source": [
    "# Gradient Based Constraint Learning Demo\n",
    "\n",
    "Licensed under the Apache License, Version 2.0.\n",
    "\n",
    "This colab explores joint learning neural networks with soft constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jPaQDSFDDlfG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 11:54:28.931214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:54:28.931232: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq7SE1IAvBAP"
   },
   "source": [
    "# Dataset and Task\n",
    "\n",
    "We test and validate our system over a common fairness dataset and task: [Adult Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income). This data was extracted from the [1994 Census bureau database](http://www.census.gov/en.html) by Ronny Kohavi and Barry Becker. Our analysis aims at learning a model that does not bias predictions towards men over 50K through soft constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t5dqEvbHu1s5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 4649844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20720/1001939022.py:23: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  train_df = pd.read_csv(csv_file, names=_COLUMNS, sep=r'\\s*,\\s*', na_values=\"?\").dropna(how=\"any\", axis=0)\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20720/1001939022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# ========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_TRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_COLUMNS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'\\s*,\\s*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"any\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_TEST_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 )\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2407\u001b[0;31m             ) = self._infer_columns()\n\u001b[0m\u001b[1;32m   2408\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2786\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2788\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2859\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2861\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_for_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m                 \u001b[0morig_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_iter_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m   3014\u001b[0m         \"\"\"\n\u001b[1;32m   3015\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_bad_lines\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2544\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2545\u001b[0m                 \u001b[0mpat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         raise errors.PermissionDeniedError(None, None,\n\u001b[1;32m     79\u001b[0m                                            \"File isn't open for reading\")\n\u001b[0;32m---> 80\u001b[0;31m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0m\u001b[1;32m     81\u001b[0m           compat.path_to_str(self.__name), 1024 * 512)\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# Constants\n",
    "# ========================================================================\n",
    "_TRAIN_PATH = ''\n",
    "_TEST_PATH = ''\n",
    "\n",
    "_COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "\n",
    "# ========================================================================\n",
    "# Seed Data\n",
    "# ========================================================================\n",
    "SEED = random.randint(-10000000, 10000000)\n",
    "print(\"Seed: %d\" % SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ========================================================================\n",
    "# Load Data\n",
    "# ========================================================================\n",
    "with tf.io.gfile.GFile(_TRAIN_PATH, 'r') as csv_file:\n",
    "  train_df = pd.read_csv(csv_file, names=_COLUMNS, sep=r'\\s*,\\s*', na_values=\"?\").dropna(how=\"any\", axis=0)\n",
    "\n",
    "with tf.io.gfile.GFile(_TEST_PATH, 'r') as csv_file:\n",
    "  test_df = pd.read_csv(csv_file, names=_COLUMNS, skiprows=[0], sep=r'\\s*,\\s*', na_values=\"?\").dropna(how=\"any\", axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8U7bmZqFPUv"
   },
   "source": [
    "# Feature Columns\n",
    "\n",
    "The following code was taken from [intro_to_fairness](https://colab.sandbox.google.com/notebooks/mlcc/intro_to_fairness.ipynb#scrollTo=tAG5hUJwx725). In short, Tensorflow requires a mapping of data and so every column is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "hOGqN4iKEsJp"
   },
   "outputs": [],
   "source": [
    "#@title Prepare Dataset\n",
    "# ========================================================================\n",
    "# Categorical Feature Columns\n",
    "# ========================================================================\n",
    "# Unknown length\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"occupation\", hash_bucket_size=1000)\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "# Known length\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"gender\", [\"Female\", \"Male\"])\n",
    "race = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"race\", [\n",
    "        \"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"\n",
    "    ])\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"education\", [\n",
    "        \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "        \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "        \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "        \"Preschool\", \"12th\"\n",
    "    ])\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"marital_status\", [\n",
    "        \"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n",
    "        \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\"\n",
    "    ])\n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"relationship\", [\n",
    "        \"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n",
    "        \"Other-relative\"\n",
    "    ])\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"workclass\", [\n",
    "        \"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n",
    "        \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\"\n",
    "    ])\n",
    "\n",
    "# ========================================================================\n",
    "# Numeric Feature Columns\n",
    "# ========================================================================\n",
    "age = tf.feature_column.numeric_column(\"age\")\n",
    "age_buckets = tf.feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "fnlwgt = tf.feature_column.numeric_column(\"fnlwgt\")\n",
    "education_num = tf.feature_column.numeric_column(\"education_num\")\n",
    "capital_gain = tf.feature_column.numeric_column(\"capital_gain\")\n",
    "capital_loss = tf.feature_column.numeric_column(\"capital_loss\")\n",
    "hours_per_week = tf.feature_column.numeric_column(\"hours_per_week\")\n",
    "\n",
    "# ========================================================================\n",
    "# Specify Features\n",
    "# ========================================================================\n",
    "deep_columns = [\n",
    "    tf.feature_column.indicator_column(workclass),\n",
    "    tf.feature_column.indicator_column(education),\n",
    "    tf.feature_column.indicator_column(age_buckets),\n",
    "    tf.feature_column.indicator_column(gender),\n",
    "    tf.feature_column.indicator_column(relationship),\n",
    "    tf.feature_column.embedding_column(native_country, dimension=8),\n",
    "    tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "]\n",
    "\n",
    "features = {\n",
    "  'age': tf.keras.Input(shape=(1,), name='age'),\n",
    "  'education': tf.keras.Input(shape=(1,), name='education', dtype=tf.string),\n",
    "  'gender': tf.keras.Input(shape=(1,), name='gender', dtype=tf.string),\n",
    "  'native_country': tf.keras.Input(shape=(1,), name='native_country', dtype=tf.string),\n",
    "  'occupation': tf.keras.Input(shape=(1,), name='occupation', dtype=tf.string),\n",
    "  'relationship': tf.keras.Input(shape=(1,), name='relationship', dtype=tf.string),\n",
    "  'workclass': tf.keras.Input(shape=(1,), name='workclass', dtype=tf.string),\n",
    "}\n",
    "\n",
    "# ========================================================================\n",
    "# Create Dataset\n",
    "# ========================================================================\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=512):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('income_bracket').apply(lambda x: \">50K\" in x).astype(int)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "7SCi3pLhBANM"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions\n",
    "def confusion_matrix(predictions, labels, threshold=0.5):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        if prediction > threshold:\n",
    "            if label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    else:\n",
    "        if label == 0:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    return (tp, tn, fp, fn)\n",
    "\n",
    "def remove_group(dataframe, predictions, group):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe['predictions'] = predictions\n",
    "    dataframe = dataframe[dataframe.gender != group]\n",
    "\n",
    "    group_predictions = dataframe.pop('predictions')\n",
    "\n",
    "    return dataframe, group_predictions\n",
    "\n",
    "def print_accuracy(dataframe, predictions, threshold=0.5):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('income_bracket').apply(lambda x: \">50K\" in x).astype(int)\n",
    "\n",
    "    tp, tn, fp, fn = confusion_matrix(predictions, labels, threshold=threshold)\n",
    "    print(\"True Positives: %d True Negatives: %d False Positives %d False Negatives: %d\" % (tp, tn, fp, fn))\n",
    "    print(\"Accuracy: %0.5f\" % ((tp+tn) / (tp + tn + fp + fn)))\n",
    "    print(\"Positive Accuracy: %0.5f\" % (tp / (tp + fp)))\n",
    "    print(\"Negative Accuracy: %0.5f\" % (tn / (tn + fn)))\n",
    "    print(\"Percentage Predicted over >50K: %0.5f\" % (((tp + fp) / (tp + tn + fp + fn)) * 100))\n",
    "\n",
    "    return (tp, tn, fp, fn)\n",
    "\n",
    "def parity(m_tp, m_fp, m_tn, m_fn, f_tp, f_fp, f_tn, f_fn):\n",
    "    return ((m_tp + m_fp) / (m_tp + m_tn + m_fp + m_fn)) - ((f_tp + f_fp) / (f_tp + f_tn + f_fp + f_fn))\n",
    "\n",
    "def print_title(title, print_length=50):\n",
    "    print(('-' * print_length) + '\\n' + title + '\\n' + ('-' * print_length))\n",
    "\n",
    "def print_analysis(train_df, train_predictions, test_df, test_predictions):\n",
    "    print_title(\"Train Accuracy\")\n",
    "    print_accuracy(train_df, train_predictions)\n",
    "\n",
    "    print_title(\"Full Test Accuracy\")\n",
    "    print_accuracy(test_df, test_predictions)\n",
    "\n",
    "    print_title(\"Male Test Accuracy\")\n",
    "    male_df, male_pred = remove_group(test_df, test_predictions, \"Female\")\n",
    "    m_tp, m_tn, m_fp, m_fn = print_accuracy(male_df, male_pred)\n",
    "\n",
    "    print_title(\"Female Test Accuracy\")\n",
    "    female_df, female_pred = remove_group(test_df, test_predictions, \"Male\")\n",
    "    f_tp, f_tn, f_fp, f_fn = print_accuracy(female_df, female_pred)\n",
    "\n",
    "    print_title(\"Parity\")\n",
    "    print(parity(m_tp, m_fp, m_tn, m_fn, f_tp, f_fp, f_tn, f_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8brN9Ppx3--"
   },
   "source": [
    "# Create and Run Non-Constrained Neural Model\n",
    "\n",
    "Defining our neural model that will be used as a comparison. Note: this model was purposfully designed to be simplistic, as it is trying to highlight the benifit to learning with soft constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1k0_BxR9Ls9X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 11:55:49.884441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 11:55:49.885220: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885270: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885313: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885357: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885399: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885481: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885522: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:55:49.885531: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-12 11:55:49.885942: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20720/1530568604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbaseline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def build_model(feature_columns, features):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(1024, activation='relu')(feature_layer(features))\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(512, activation='relu')(hidden_layer_1)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer_2)\n",
    "\n",
    "    model = tf.keras.Model([v for v in features.values()], output)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mse',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "baseline_model = build_model(deep_columns, features)\n",
    "baseline_model.fit(df_to_dataset(train_df), epochs=50)\n",
    "\n",
    "test_predictions = baseline_model.predict(df_to_dataset(test_df, shuffle=False))\n",
    "baseline_model.evaluate(df_to_dataset(test_df))\n",
    "\n",
    "train_predictions = baseline_model.predict(df_to_dataset(train_df, shuffle=False))\n",
    "baseline_model.evaluate(df_to_dataset(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji1jtzBvyLTo"
   },
   "source": [
    "# Analyze Non-Constrained Results\n",
    "\n",
    "For this example we look at the fairness constraint that the protected group (gender) should have no predictive difference between classes. In this situation this means that the ratio of positive predictions should be the same between male and female.\n",
    "\n",
    "Note: this is by no means the only fairness constraint needed to have a fair model, and in fact can result in some doubious results (as seen in the follwoing section).\n",
    "\n",
    "The results do clearly show a skew in ratios as males are have a higher ratio of >50k predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWQJpfMryH1l"
   },
   "outputs": [],
   "source": [
    "print_analysis(train_df, train_predictions, test_df, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpFJjNOfyurV"
   },
   "source": [
    "# Define Constraints\n",
    "\n",
    "This requires a constrained loss function and a custom train step within the keras model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d3jk0Pu8hTP"
   },
   "outputs": [],
   "source": [
    "def constrained_loss(data, logits, threshold=0.5, weight=3):\n",
    "    \"\"\"Linear constrained loss for equal ratio prediction for the protected group.\n",
    "\n",
    "    The constraint: (#Female >50k / #Total Female) - (#Male >50k / #Total Male)\n",
    "    This constraint penalizes predictions between the protected group (gender),\n",
    "    such that the ratio between all classes must be the same.\n",
    "    An important note: to maintian differentability we do not use #Female >50k\n",
    "    (which requires a round operation), instead we set values below the threshold\n",
    "    to zero, and sum the logits.\n",
    "\n",
    "    Args:\n",
    "    data: Input features.\n",
    "    logits: Predictions made in the logit.\n",
    "    threshold: Binary threshold for predicting positive and negative labels.\n",
    "    weight: Weight of the constrained loss.\n",
    "\n",
    "    Returns:\n",
    "    A scalar loss of the constraint violations.\n",
    "    \"\"\"\n",
    "    gender_label, gender_idx, gender_count = tf.unique_with_counts(data['gender'], out_idx=tf.int32, name=None)\n",
    "    cut_logits = tf.reshape(tf.cast(logits > threshold, logits.dtype) * logits, [-1])\n",
    "\n",
    "    def f1():\n",
    "        return gender_idx\n",
    "    def f2():\n",
    "        return tf.cast(tf.math.logical_not(tf.cast(gender_idx, tf.bool)), tf.int32)\n",
    "\n",
    "    # Load male indexes as ones and female indexes to zeros.\n",
    "    male_index = tf.cond(tf.reduce_all(tf.equal(gender_label, tf.constant([\"Male\", \"Female\"]))), f1, f2)\n",
    "    # Cast the integers to float32 to do a multiplication with the logits.\n",
    "    male_index = tf.cast(male_index, tf.float32)\n",
    "    # (#Male > 50k / #Total Male)\n",
    "    male_prob = tf.divide(tf.reduce_sum(tf.multiply(cut_logits, male_index)), tf.reduce_sum(male_index))\n",
    "\n",
    "    # Flip all female indexes to one and male indexes to zeros.\n",
    "    female_index = tf.math.logical_not(tf.cast(male_index, tf.bool))\n",
    "    # Cast the integers to float32 to do a multiplication with the logits.\n",
    "    female_index = tf.cast(female_index, tf.float32)\n",
    "    # (#Female > 50k / #Total Female)\n",
    "    female_prob = tf.divide(tf.reduce_sum(tf.multiply(cut_logits, female_index)), tf.reduce_sum(female_index))\n",
    "\n",
    "    # Since tf.math.abs is not differentable, separate the loss into two hinges.\n",
    "    loss = tf.add(tf.maximum(male_prob - female_prob, 0.0), tf.maximum(female_prob - male_prob, 0.0))\n",
    "    return tf.multiply(loss, weight)\n",
    "\n",
    "class StructureModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        features, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(features, training=True)\n",
    "            standard_loss = self.compiled_loss(labels, logits, regularization_losses=self.losses)\n",
    "            constraint_loss = constrained_loss(features, logits)\n",
    "            loss =  standard_loss + constraint_loss\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoZCYYTwzidW"
   },
   "source": [
    "# Build and Run Constrained Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ3-B8Bjzl30"
   },
   "outputs": [],
   "source": [
    "def build_constrained_model(feature_columns, features):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(1024, activation='relu')(feature_layer(features))\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(512, activation='relu')(hidden_layer_1)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer_2)\n",
    "\n",
    "    model = StructureModel([v for v in features.values()], output)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mse',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "constrained_model = build_constrained_model(deep_columns, features)\n",
    "constrained_model.fit(df_to_dataset(train_df), epochs=50)\n",
    "\n",
    "test_predictions = constrained_model.predict(df_to_dataset(test_df, shuffle=False))\n",
    "constrained_model.evaluate(df_to_dataset(test_df))\n",
    "\n",
    "train_predictions = constrained_model.predict(df_to_dataset(train_df, shuffle=False))\n",
    "constrained_model.evaluate(df_to_dataset(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhvI8j62ltWt"
   },
   "source": [
    "# Analyze Constrained Results\n",
    "\n",
    "Ideally this constraint should correct the ratio imbalance between the protected groups (gender). This means our parity should be very close to zero.\n",
    "\n",
    "Note: This constraint does not mean the neural classifier is guaranteed to generalize and make better predictions. It is more likely to attempt to balance the class prediction ratio in the simplest fashion (resulting in a worse accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQrWoCYmlvbA"
   },
   "outputs": [],
   "source": [
    "print_analysis(train_df, train_predictions, test_df, test_predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "gradient_based_constraint_learning_demo.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1ri2Frd6c1Ho7lC7_b91X7q6onP5Yn1uP",
     "timestamp": 1633637940232
    },
    {
     "file_id": "/piper/depot/google3/experimental/users/cfpryor/neural_learning_with_soft_constraints.ipynb",
     "timestamp": 1627691047339
    },
    {
     "file_id": "/piper/depot/google3/experimental/users/cfpryor/neural_learning_with_soft_constraints.ipynb?workspaceId=cfpryor:cfpryor_intern2021::citc",
     "timestamp": 1626731982182
    },
    {
     "file_id": "1s_gTqW9Oy9G1pU7VrtIUNlL3SUJcGAZP",
     "timestamp": 1626478300089
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
