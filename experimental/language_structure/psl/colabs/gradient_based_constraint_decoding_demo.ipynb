{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPZaHYB21OQi"
   },
   "source": [
    "# Gradient Based Constrained Decoding Demo\n",
    "\n",
    "Licensed under the Apache License, Version 2.0.\n",
    "\n",
    "This method is based upon [Gradient-based Inference for Networks\n",
    "with Output Constraints](https://arxiv.org/pdf/1707.08608.pdf) by Lee et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JXWCFnte1Qz_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import inference.gradient_decoding as gradient_decoding\n",
    "import scripts.multiwoz_synthetic_data_util as data_util\n",
    "import scripts.multiwoz_synthetic_gradient_decoding_util as gradient_decoding_util\n",
    "import scripts.util as util\n",
    "import models.multiwoz_synthetic.psl_model as psl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKMZVzuP1Wm3"
   },
   "source": [
    "# Dataset and Task\n",
    "\n",
    "We study constrained decoding through the task of dialog structure prediction. Dialog structure is the high level representation of the flow of a dialog, where nodes represent abstract topics or dialog acts that statements would fit into and edges represent topic changes.\n",
    "\n",
    "To verify our method we ideally would like to test it over a multi-goal oriented dialog corpus such as [MultiWoZ 2.0](https://arxiv.org/pdf/1907.01669.pdf), created by Mihail Eric et. al. Unfortunately, this corpus does not have a ground truth dialog structure, therefore, we use a [Synthetic Multi-WoZ](https://almond-static.stanford.edu/papers/multiwoz-acl2020.pdf) dataset created by Giovanni Campagna et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7q08dilwptzi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 2977202\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# Constants\n",
    "# ========================================================================\n",
    "DATA_PATH = ''\n",
    "RULE_WEIGHTS = gradient_decoding_util.RULE_WEIGHTS\n",
    "RULE_NAMES = gradient_decoding_util.RULE_NAMES\n",
    "\n",
    "ALPHAS = [0.1]\n",
    "GRAD_STEPS = [10, 50, 100, 500]\n",
    "LEARNING_RATES = [0.0001, 0.0005, 0.001, 0.01]\n",
    "\n",
    "# ========================================================================\n",
    "# Seed Data\n",
    "# ========================================================================\n",
    "SEED = random.randint(-10000000, 10000000)\n",
    "print(\"Seed: %d\" % SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ========================================================================\n",
    "# Load Data and Config\n",
    "# ========================================================================\n",
    "DATA = data_util.load_json(DATA_PATH)\n",
    "CONFIG = gradient_decoding_util.CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLCc30du1huD"
   },
   "source": [
    "# Neural Model\n",
    "\n",
    "Below is a simple neural model for supervised structure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "zwRtAijRqSRu"
   },
   "outputs": [],
   "source": [
    "#@title Create Neural Model\n",
    "def build_model(input_size, learning_rate=0.001):\n",
    "    \"\"\"Build simple neural model for class prediction.\"\"\"\n",
    "    input_layer = tf.keras.layers.Input(input_size)\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(1024)(input_layer)\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(\n",
    "      512, activation='sigmoid')(\n",
    "          hidden_layer_1)\n",
    "    output = tf.keras.layers.Dense(\n",
    "      9, activation='softmax',\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(1.0))(\n",
    "          hidden_layer_2)\n",
    "\n",
    "    model = tf.keras.Model(input_layer, output)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 11:39:29.128471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-12 11:39:29.129262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129312: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129436: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129559: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-11-12 11:39:29.129569: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-12 11:39:29.129885: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = gradient_decoding_util.prepare_dataset(DATA, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e9SzqHwKrdIF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 10ms/step - loss: 17.4626 - accuracy: 0.2611\n",
      "Categorical Accuracy: 0.4948\n",
      "Class: accept          Precision: 0.0000  Recall: 0.0000  F1: 0.0000\n",
      "Class: cancel          Precision: 0.0114  Recall: 0.1176  F1: 0.0207\n",
      "Class: end             Precision: 0.0194  Recall: 0.1552  F1: 0.0344\n",
      "Class: greet           Precision: 0.0282  Recall: 0.0217  F1: 0.0245\n",
      "Class: info_question   Precision: 0.0043  Recall: 0.0909  F1: 0.0082\n",
      "Class: init_request    Precision: 0.7613  Recall: 0.8632  F1: 0.8091\n",
      "Class: insist          Precision: 0.5226  Recall: 0.0548  F1: 0.0992\n",
      "Class: second_request  Precision: 0.5995  Recall: 0.6464  F1: 0.6220\n",
      "Class: slot_question   Precision: 0.0000  Recall: 0.0000  F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def run_non_constrained(train_ds, test_ds, test_labels, config, learning_rate):\n",
    "    model = build_model([config['max_dialog_size'], config['max_utterance_size']], learning_rate=learning_rate)\n",
    "    model.fit(train_ds, epochs=config['train_epochs'])\n",
    "\n",
    "    logits = model.predict(test_ds)\n",
    "    predictions = tf.math.argmax(logits, axis=-1)\n",
    "\n",
    "    confusion_matrix = util.class_confusion_matrix(predictions, test_labels, config)\n",
    "    metrics, cat_accuracy = util.print_metrics(confusion_matrix)\n",
    "\n",
    "    return model, metrics, cat_accuracy\n",
    "\n",
    "test_model, metrics, cat_accuracy = run_non_constrained(train_ds, test_ds, DATA['train_truth_dialog'], CONFIG, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMQtBEO41eyh"
   },
   "source": [
    "# Gradient Based Constraint Decoding\n",
    "\n",
    "Rules:\n",
    "\n",
    "1. !FirstStatement(S) -> !State(S, 'greet')\n",
    "2. FirstStatement(S) & HasGreetWord(S) -> State(S, 'greet')\n",
    "3. FirstStatement(S) & !HasGreetWord(S) -> State(S, 'init_request')\n",
    "4. PreviousStatement(S1, S2) & State(S2, 'init_request') -> State(S1, 'second_request')\n",
    "5. PreviousStatement(S1, S2) & !State(S2, 'greet') -> !State(S1, 'init_request')\n",
    "6. PreviousStatement(S1, S2) & State(S2, 'greet') -> State(S1, 'init_request')\n",
    "7. LastStatement(S) & HasEndWord(S) -> State(S, 'end')\n",
    "8. LastStatement(S) & HasAcceptWord(S) -> State(S, 'accept')\n",
    "9. PreviousStatement(S1, S2) & State(S1, 'end') & HasCancelWord(S2) -> State(S2, 'cancel')\n",
    "10. PreviousStatement(S1, S2) & State(S2, 'second_request') & HasInfoQuestionWord(S1) -> State(S1, 'info_question')\n",
    "11. LastStatement(S) & HasInsistWord(S) -> State(S, 'insist')\n",
    "12. PreviousStatement(S1, S2) & State(S2, 'second_request') & HasSlotQuestionWord(S1) -> State(S1, 'slot_question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oCj-VXXNsTX2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.metrics.Mean object at 0x7fd1c9c63850> 12.6262856\n",
      "<keras.metrics.MeanMetricWrapper object at 0x7fd1c9b74d90> 0.548742831\n",
      "Categorical Accuracy: 0.5753\n",
      "Class: accept          Precision: 0.7939  Recall: 0.0882  F1: 0.1587\n",
      "Class: cancel          Precision: 0.0000  Recall: 0.0000  F1: 0.0000\n",
      "Class: end             Precision: 0.7763  Recall: 0.4925  F1: 0.6027\n",
      "Class: greet           Precision: 1.0000  Recall: 0.9167  F1: 0.9565\n",
      "Class: info_question   Precision: 0.0000  Recall: 0.0000  F1: 0.0000\n",
      "Class: init_request    Precision: 0.7460  Recall: 0.9286  F1: 0.8274\n",
      "Class: insist          Precision: 0.8316  Recall: 0.3468  F1: 0.4895\n",
      "Class: second_request  Precision: 0.5551  Recall: 0.8269  F1: 0.6642\n",
      "Class: slot_question   Precision: 0.1990  Recall: 0.2123  F1: 0.2054\n"
     ]
    }
   ],
   "source": [
    "def run_constrained(test_model, rule_weights, rule_names, test_ds, test_labels, config, alpha, grad_step):\n",
    "    psl_constraints = psl_model.PSLModelMultiWoZ(rule_weights, rule_names, config=config)\n",
    "    logits = gradient_decoding.evaluate_constrained_model(test_model, test_ds, psl_constraints, grad_steps=grad_step, alpha=alpha)\n",
    "    predictions = tf.math.argmax(tf.concat(logits, axis=0), axis=-1)\n",
    "\n",
    "    confusion_matrix = util.class_confusion_matrix(predictions, test_labels, config)\n",
    "    metrics, cat_accuracy = util.print_metrics(confusion_matrix)\n",
    "\n",
    "    return predictions, metrics, cat_accuracy\n",
    "\n",
    "predictions, metrics, cat_accuracy = run_constrained(test_model, RULE_WEIGHTS, RULE_NAMES, test_ds, DATA['test_truth_dialog'], CONFIG, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI4uztt5Wht_"
   },
   "source": [
    "# Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TdfArRMmuwlZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def recover_utterances(dialog, vocab_map):\n",
    "    sentences = []\n",
    "    for utterance in dialog:\n",
    "        sentence = ''\n",
    "\n",
    "    for word in utterance:\n",
    "        if word in [0, -1, -2, -3]:\n",
    "            continue\n",
    "        sentence += ' ' + vocab_map[word]\n",
    "\n",
    "    if sentence != '':\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def print_dialog(dialog_index, vocab_map, class_map, data, predictions):\n",
    "    vocab_map = {v: k for k, v in vocab_map.items()}\n",
    "    class_map = {v: k for k, v in class_map.items()}\n",
    "    utterances = recover_utterances(test_data[0][dialog_index], vocab_map)\n",
    "\n",
    "    for utterance_index in range(len(utterances)):\n",
    "        key = predictions[dialog_index][utterance_index]\n",
    "        print(\"Prediction: %s Utterance: %s\" % (class_map[int(key)].ljust(15), utterances[utterance_index]))\n",
    "\n",
    "def run_analysis(test_data, predictions):\n",
    "    print(\"\\nDialog Greet\")\n",
    "    print('-' * 50)\n",
    "    print_dialog(27, DATA['vocab_mapping'], CONFIG['class_map'], test_data, predictions)\n",
    "    print(\"\\nDialog End\")\n",
    "    print('-' * 50)\n",
    "    print_dialog(6, DATA['vocab_mapping'], CONFIG['class_map'], test_data, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbdDX4atVdID"
   },
   "source": [
    "# Run Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Vq80pblMIhci"
   },
   "outputs": [],
   "source": [
    "def run_grid(train_ds, test_ds, test_data, test_labels, rule_weights, rule_names, vocab_mapping, config, alphas, grad_steps, learning_rates):\n",
    "    character_size = 80\n",
    "\n",
    "    constrained_metrics = []\n",
    "    non_constrained_metrics = []\n",
    "    constrained_cat_accuracies = []\n",
    "    non_constrained_cat_accuracies = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for grad_step in grad_steps:\n",
    "            for learning_rate in learning_rates:\n",
    "                print('\\n' + '=' * character_size)\n",
    "                print(\"Running: Alpha - %0.5f   Gradient Steps - %d   Learning Rate - %0.5f\" % (alpha, grad_step, learning_rate))\n",
    "                print('=' * character_size)\n",
    "\n",
    "                print('\\nNon-Constrained')\n",
    "                print('-' * character_size)\n",
    "                test_model, metrics, cat_accuracy = run_non_constrained(train_ds, test_ds, DATA['test_truth_dialog'], config, learning_rate=learning_rate)\n",
    "                non_constrained_metrics.append(metrics)\n",
    "                non_constrained_cat_accuracies.append(cat_accuracy)\n",
    "\n",
    "                print('\\nConstrained')\n",
    "                print('-' * character_size)\n",
    "                predictions, metrics, cat_accuracy = run_constrained(test_model, rule_weights, rule_names, test_ds, DATA['test_truth_dialog'], config, alpha=alpha, grad_step=grad_step)\n",
    "                constrained_metrics.append(metrics)\n",
    "                constrained_cat_accuracies.append(cat_accuracy)\n",
    "\n",
    "                print(\"\\nDialog Greet\")\n",
    "                print('-' * 50)\n",
    "                print_dialog(11, DATA['vocab_mapping'], config['class_map'], test_data, predictions)\n",
    "                print(\"\\nDialog End\")\n",
    "                print('-' * 50)\n",
    "                print_dialog(6, DATA['vocab_mapping'], config['class_map'], test_data, predictions)\n",
    "\n",
    "    return non_constrained_metrics, constrained_metrics, non_constrained_cat_accuracies, constrained_cat_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "gradient_based_constraint_decoding_demo.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1xEyh8B35U7Zhe11oYlIOSo33vSPlxxuA",
     "timestamp": 1633637977853
    }
   ]
  },
  "kernelspec": {
   "display_name": "PyCharm (psl)",
   "language": "python",
   "name": "pycharm-ca696f6f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}